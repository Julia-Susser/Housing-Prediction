{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('robustscaler', RobustScaler()),\n",
       "                ('elasticnet',\n",
       "                 ElasticNet(alpha=0.0005, l1_ratio=0.9, random_state=3))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score # to split the data\n",
    "from sklearn.metrics import explained_variance_score, median_absolute_error, r2_score, mean_squared_error #To evaluate our model\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, fbeta_score #To evaluate our model\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split # Model evaluation\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler # Preprocessing\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet, RANSACRegressor, SGDRegressor, HuberRegressor, BayesianRidge # Linear models\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, AdaBoostRegressor, GradientBoostingRegressor, ExtraTreesRegressor  # Ensemble methods\n",
    "from xgboost import XGBRegressor, plot_importance # XGBoost\n",
    "from sklearn.svm import SVR, SVC, LinearSVC  # Support Vector Regression\n",
    "from sklearn.tree import DecisionTreeRegressor # Decision Tree Regression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline # Streaming pipelines\n",
    "from sklearn.decomposition import KernelPCA, PCA # Dimensionality reduction\n",
    "from sklearn.feature_selection import SelectFromModel # Dimensionality reduction\n",
    "from sklearn.model_selection import learning_curve, validation_curve, GridSearchCV # Model evaluation\n",
    "from sklearn.base import clone # Clone estimator\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "\n",
    "make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "seed = 5\n",
    "n_folds =5\n",
    "scoring='r2'\n",
    "model = make_pipeline(RobustScaler(), LinearRegression())\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "cross_val_score(model, train, y_train, cv= kfold,\n",
    "                                 scoring=scoring, n_jobs=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train)\n",
    "scaled_train = scaler.transform(train)\n",
    "model = KNeighborsRegressor()\n",
    "cross_val_score(model, scaled_train, y_train, cv= kfold,\n",
    "                                 scoring=scoring, n_jobs=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X, y = make_classification(random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "pipe.fit(X_train, y_train)  # apply scaling on training data\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In terms of machine learning, Clf is an estimator instance, which is used to store model. We use clf to store trained model values, which are further used to predict value, based on the previously stored weights\n",
    "clfs = {}\n",
    "\n",
    "# SVC\n",
    "clfs['SVC'] = {\n",
    "    'instance': SVC(probability=True),\n",
    "    'params': [\n",
    "        {'kernel': ['linear'], 'C': [0.01, 0.05, 0.1, 0.5, 1]},\n",
    "        {'kernel': ['rbf'], 'C': [1, 10, 50, 100, 250], 'gamma': [0.1, 0.2, 0.3]}\n",
    "    ]    \n",
    "}\n",
    "\n",
    "# RandomForest\n",
    "clfs['RandomForestClassifier'] = {\n",
    "    'instance': RandomForestClassifier(n_jobs=-1),\n",
    "    'params': {        \n",
    "        'n_estimators': [25, 50, 100],\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_depth': [10, 25, 50, None]\n",
    "    }\n",
    "}\n",
    "\n",
    "# LogisticRegression\n",
    "clfs['LogisticRegression'] = {\n",
    "    'instance': LogisticRegression(max_iter=200, n_jobs=-1),\n",
    "    'params': [\n",
    "            {'penalty': ['l2'], 'C': [0.1, 0.5, 1, 5, 10]},\n",
    "            {'penalty': ['l1'], 'solver': ['liblinear', 'saga'], 'C': [0.1, 0.5, 1, 5, 10]},\n",
    "            {'penalty': ['elasticnet'], 'C': [0.1, 0.5, 1, 5, 10], 'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]}\n",
    "        ]\n",
    "}\n",
    "\n",
    "\n",
    "for clf_name, clf in clfs.items():\n",
    "    print('<{}>'.format(clf_name))\n",
    "    print('  training ...')\n",
    "    \n",
    "   \n",
    "    gs = GridSearchCV(clf['instance'], param_grid=clf['params'], cv=5, n_jobs=-1)\n",
    "    gs.fit(X, y)\n",
    "    \n",
    "    print('  best_score: {:.3f}'.format(gs.best_score_))\n",
    "    print('  best_params: {}'.format(gs.best_params_))\n",
    "    \n",
    " \n",
    "    clfs[clf_name]['best_params'] = gs.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
